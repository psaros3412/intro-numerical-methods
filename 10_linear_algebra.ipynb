{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table>\n",
    " <tr align=left><td><img align=left src=\"./images/CC-BY.png\">\n",
    " <td>Text provided under a Creative Commons Attribution license, CC-BY. All code is made available under the FSF-approved MIT license. (c) Kyle T. Mandli</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical Linear Algebra\n",
    "\n",
    "Numerical methods for linear algebra problems lies at the heart of many numerical approaches and is something we will spend some time on.  Roughly we can break down probablems that we would like to solve into two general problems, solving a system of equations\n",
    "\n",
    "$$A x = b$$\n",
    "\n",
    "and solving the eigenvalue problem\n",
    "\n",
    "$$A v = \\lambda v.$$\n",
    "\n",
    "We will take each in turn, evaluate some of the fundamental properties and methods for solving these problems, and gain some understanding of when they fail.\n",
    "\n",
    "\u001f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General Problem Specification\n",
    "\n",
    "Before we dive in lets consider some of the pivotal problems that numerical methods for linear algebra tries to address.  \n",
    "\n",
    "For this discussion we will be using the common notation $m \\times n$ to denote the dimensions of a matrix $A$.  The $m$ refers to the number of rows and $n$ the number of columns.  If a matrix is square, i.e. $m = n$, then we will use the notation that $A$ is $m \\times m$.\n",
    "\n",
    "### Systems of Equations\n",
    "\n",
    "We have $m$ equations for $m$ unknowns.\n",
    "\n",
    "$$A x = b$$\n",
    "\n",
    "#### Example: Vandermonde Matrix\n",
    "\n",
    "We have data $(x_i, y_i), ~~ i = 1, 2, \\ldots, m$ that we want to fit a polynomial of order $m-1$.  Sovling the linear system $A p = y$ does this for us where\n",
    "\n",
    "$$A = \\begin{bmatrix}\n",
    "    1 & x_1 & x_1^2 & \\cdots & x_1^{m-1} \\\\\n",
    "    1 & x_2 & x_2^2 & \\cdots & x_2^{m-1} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
    "    1 & x_m & x_m^2 & \\cdots & x_m^{m-1}\n",
    "\\end{bmatrix} ~~~~~ y = \\begin{bmatrix}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "and $p$ are the coefficients of the interpolating polynomial $\\mathcal{P}_N(x) = p_0 + p_1 x + p_2 x^2 + \\cdots + p_m x^{m-1}$.\n",
    "\n",
    "#### Example: Linear least squares 1\n",
    "\n",
    "In a similar case as above, say we want to fit a paritcular function (could be a polynomial) to a given number of data points except in this case we have more data points than free parameters.  In the case of polynomials this could be the same as saying we have $m$ data points but only want to fit a $n$th order polynomial through the data where $n \\leq m - 1$.  One of the common approaches to this problem is to minimize the \"least-squares\" error between the data and the resulting function:\n",
    "\n",
    "$$E = \\left( \\sum^m_{i=1} |y_i - f(x_i)|^2 \\right )^{1/2}.$$\n",
    "\n",
    "But how do we do this if our matrix $A$ is now $m \\times n$ and looks like\n",
    "\n",
    "$$A = \\begin{bmatrix}\n",
    "    1 & x_1 & x_1^2 & \\cdots & x_1^{n} \\\\\n",
    "    1 & x_2 & x_2^2 & \\cdots & x_2^{n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
    "    1 & x_m & x_m^2 & \\cdots & x_m^{n}\n",
    "\\end{bmatrix}?$$\n",
    "\n",
    "Turns out if we solve the system\n",
    "\n",
    "$$A^T A x = A^T b$$\n",
    "\n",
    "we can gaurantee that the error is minimized in the least-squares sense.\n",
    "\n",
    "#### Example:  Linear least squares 2\n",
    "\n",
    "Try and fit a line through the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYAAAkPT/AUeSWbdGuOFlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107c3c050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Linear Least Squares Problem\n",
    "\n",
    "x = numpy.array([        0.0, 0.11111111, 0.22222222, 0.33333333, 0.44444444, \n",
    "                  0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.0])\n",
    "y = numpy.array([ 1.87498549,  1.31037444,  1.23798973,  1.96912525,  1.7150103 ,\n",
    "                  2.07664549,  1.53174786,  1.96478464,  2.31023704,  1.59167973])\n",
    "\n",
    "A = numpy.ones((x.shape[0], 2))\n",
    "A[:, 1] = x\n",
    "p = numpy.linalg.lstsq(A, y)[0]\n",
    "\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axes.plot(x, y, 'ko')\n",
    "axes.plot(x, p[0] + p[1] * x, 'r')\n",
    "axes.set_title(\"Least Squares Fit to Data\")\n",
    "axes.set_xlabel(\"$x$\")\n",
    "axes.set_ylabel(\"$f(x)$ and $y_i$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Eigenproblems\n",
    "\n",
    "Eigenproblems come up in a variety of contexts and often are integral to many problem of scientific and engineering interest.  As a review, if $A \\in \\mathbb{C}^{m\\times m}$ (a square matrix with complex values), a non-zero vector $v\\in\\mathbb{C}^m$ is an **eigenvector** of $A$ with a corresponding **eigenvalue** $\\lambda \\in \\mathbb{C}$ if \n",
    "\n",
    "$$A v = \\lambda v.$$\n",
    "\n",
    "One way to interpret the eigenproblem is that we are attempting to ascertain the \"action\" of the matrix $A$ on some subspace of $\\mathbb{C}^m$ where this action acts like scalar multiplication.  This subspace is called an **eigenspace**.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Compute the eigenspace of the matrix\n",
    "\n",
    "$$A = \\begin{bmatrix}\n",
    "    1 & 2 \\\\\n",
    "    2 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Recall that we can find the eigenvalues of a matrix by computing $\\det(A - \\lambda I) = 0$.  In this case we have\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    A - \\lambda I &= \\begin{bmatrix}\n",
    "        1 & 2 \\\\\n",
    "        2 & 1\n",
    "    \\end{bmatrix} - \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\lambda\\\\\n",
    "    &= \\begin{bmatrix}\n",
    "        1 - \\lambda & 2 \\\\\n",
    "        2 & 1 - \\lambda\n",
    "    \\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "whose determinant is\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\begin{vmatrix}\n",
    "    1 - \\lambda & 2 \\\\\n",
    "    2 & 1 - \\lambda\n",
    "\\end{vmatrix} &= (1 - \\lambda) (1 - \\lambda) - 2 \\cdot 2 \\\\\n",
    "&= 1 - 2 \\lambda + \\lambda^2 - 4 \\\\\n",
    "&= \\lambda^2 - 2 \\lambda - 3.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Setting this equal to zero we can find the eigenvalues as\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\lambda &= \\frac{2 \\pm \\sqrt{4 - 4 \\cdot 1 \\cdot (-3)}}{2} \\\\\n",
    "&= 1 \\pm 2 \\\\\n",
    "&= [-1, 3]\n",
    "\\end{aligned}$$\n",
    "\n",
    "The eigenvectors then can be found by rearranging the equation to $(A - \\lambda I) v = 0$ and solving for each vector.  A trick that works most of the time is to normalize each vector such that the first entry is 1 ($v_1 = 1$):\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "        1 - \\lambda & 2 \\\\\n",
    "        2 & 1 - \\lambda\n",
    "    \\end{bmatrix} \\begin{bmatrix} 1 \\\\ v_2 \\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    1 - \\lambda + 2 v_2 &= 0 \\\\\n",
    "    v_2 &= \\frac{\\lambda - 1}{2}\n",
    "\\end{aligned}$$\n",
    "\n",
    "and just to make sure\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    2 + \\left(1- \\lambda \\frac{\\lambda - 1}{2}\\right) & = 0\\\\\n",
    "    (\\lambda - 1)^2 - 4 &=0\n",
    "\\end{aligned}$$\n",
    "\n",
    "which by design is satisfied by our eigenvalues.  Another sometimes easier approach is to plug-in the eigenvalues to find each corresponding eigenvector.  The eigenvectors are therefore\n",
    "\n",
    "$$v = \\begin{bmatrix}1 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix}1 \\\\ 1 \\end{bmatrix}.$$\n",
    "\n",
    "Note that these are linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamentals\n",
    "\n",
    "### Matrix-Vector Multiplication\n",
    "\n",
    "One of the most basic operations we can perform with matrices is to multiply them be a vector.  This matrix-vector product $A x = b$ is defined as\n",
    "\n",
    "$$b_i = \\sum^n_{j=1} a_{ij} x_j ~~~~~ \\text{where}~~~~ i = 1, \\ldots, m$$\n",
    "\n",
    "Writing the matrix-vector product this way we see that one interpretation of this product is that each column of $A$ is weighted by the value $x_j$, or in other words $b$ is a linear combination of the columns of $A$ where each column's weighting is $x$.\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix} ~ \\\\ ~ \\\\ b \\\\ ~ \\\\ ~ \\end{bmatrix} = \n",
    "    \\begin{bmatrix} ~ & ~ & ~ & ~ \\\\ ~ & ~ & ~ & ~  \\\\ a_1 & a_2 & \\cdots & a_n \\\\ ~ & ~ & ~ & ~  \\\\ ~ & ~ & ~ & ~ \\end{bmatrix}\n",
    "    \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = x_1 \\begin{bmatrix} ~ \\\\ ~ \\\\ a_1 \\\\ ~ \\\\ ~ \\end{bmatrix} + x_2 \\begin{bmatrix} ~ \\\\ ~ \\\\ a_2 \\\\ ~ \\\\ ~ \\end{bmatrix} + \\cdots + x_n \\begin{bmatrix} ~ \\\\ ~ \\\\ a_n \\\\ ~ \\\\ ~ \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This view will be useful later when we are trying to interpret various types of matrices.\n",
    "\n",
    "One important property of the matrix-vector product is that is a **linear** operation, also known as a **linear operator**.  This means that the for any $x, y \\in \\mathbb{C}^n$ and any $c \\in \\mathbb{C}$ we know that\n",
    "\n",
    "1. $A (x + y) = Ax + Ay$\n",
    "1. $A\\cdot (cx) = c A x$\n",
    "\n",
    "#### Example:  Vandermonde Matrix\n",
    "\n",
    "In the case where we have $m$ data points and want $m - 1$ order polynomial interpolant the matrix $A$ is a square $m \\times m$ matrix as before.  Using the above interpretation the polynomial coefficients $p$ are the weights for each of the monomials that give exactly the $y$ values of the data.\n",
    "\n",
    "#### Example:  Numerical matrix-vector multiply\n",
    "\n",
    "Write a matrix-vector multiply function and check it with the appropriate `numpy` routine.  Also verify the linearity of the matrix-vector multiply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def matrix_vector_product(A, x):\n",
    "    b = numpy.zeros((A.shape[0]))\n",
    "    for i in xrange(A.shape[0]):\n",
    "        for j in xrange(A.shape[1]):\n",
    "            b[i] += A[i, j] * x[j]\n",
    "    return b\n",
    "\n",
    "\n",
    "m = 4\n",
    "n = 3\n",
    "A = numpy.random.uniform(size=(m,n))\n",
    "x = numpy.random.uniform(size=(n))\n",
    "y = numpy.random.uniform(size=(n))\n",
    "c = numpy.random.uniform()\n",
    "b = matrix_vector_product(A, x)\n",
    "print numpy.allclose(b, numpy.dot(A, x))\n",
    "print numpy.allclose(matrix_vector_product(A, (x + y)), matrix_vector_product(A, x) + matrix_vector_product(A, y))\n",
    "print numpy.allclose(matrix_vector_product(A, c * x), c*matrix_vector_product(A, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix-Matrix Multiplication\n",
    "\n",
    "The matrix product with another matrix $A C = B$ is defined as\n",
    "$$\n",
    "    b_{ij} = \\sum^m_{k=1} a_{ik} c_{kj}.\n",
    "$$\n",
    "Again, a useful interpretation of this operation is that the product result $B$ is the a linear combination of the columns of $A$.\n",
    "\n",
    "What are the dimensions of $A$ and $C$ so that the multiplication works?\n",
    "\n",
    "#### Example:  Outer Product\n",
    "\n",
    "The product of two vectors $u \\in \\mathbb{C}^m$ and $v \\in \\mathbb{C}^n$ is a $m \\times n$ matrix where the columns are the vector $u$ multiplied by the corresponding value of $v$:\n",
    "$$\n",
    "    \\begin{bmatrix} ~ \\\\ ~ \\\\ u \\\\ ~ \\\\ ~ \\end{bmatrix} \\begin{bmatrix} v_1 & v_2 & \\cdots \\\\ v_n \\end{bmatrix} =\n",
    "    \\begin{bmatrix} ~ & ~ & ~ & ~ \\\\ ~ & ~ & ~ & ~ \\\\ v_1 u & v_2 u & \\cdots & v_n u \\\\ ~ & ~ & ~ & ~ \\\\ ~ & ~ & ~ & ~  \\end{bmatrix} = \\begin{bmatrix} v_1u_1 & \\cdots & v_n u_1 \\\\ \\vdots & ~ & \\vdots \\\\ v_1 u_m & \\cdots & v_n u_m \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Example:  Upper Triangular Multiplication\n",
    "\n",
    "Consider the multiplication of a matrix $A \\in \\mathbb{C}^{m\\times n}$ and the **upper-triangular** matrix $R$ defined as the $n \\times n$ matrix with entries $r_{ij} = 1$ for $i \\leq j$ and $r_{ij} = 0$ for $i > j$.  The product can be written as\n",
    "$$\n",
    "    \\begin{bmatrix} ~ & ~ &  \\\\ ~ & ~ &  \\\\ b_1 & \\cdots & b_n \\\\ ~ & ~ &  \\\\ ~ & ~ & ~ \\end{bmatrix} = \\begin{bmatrix} ~ & ~ &  \\\\ ~ & ~ &  \\\\ a_1 & \\cdots & a_n \\\\ ~ & ~ &  \\\\ ~ & ~ & ~ \\end{bmatrix} \\begin{bmatrix} 1 & \\cdots & 1 \\\\ ~ & \\ddots & \\vdots \\\\ ~ & ~ & 1 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The columns of $B$ are then\n",
    "$$\n",
    "    b_j = A r_j = \\sum^j_{k=1} a_k\n",
    "$$\n",
    "so that $b_j$ is the sum of the first $j$ columns of $A$.\n",
    "\n",
    "#### Example: Write Matrix-Matrix Multiplication\n",
    "\n",
    "Write a function that compute matrix-matrix multiplication and again demonstrates the following properties:\n",
    "1. $A (B + C) = AB + AC$ (for square matrices))\n",
    "1. $A (cB) = c AB$ where $c \\in \\mathbb{C}$\n",
    "1. $AB \\neq BA$ in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def matrix_matrix_product(A, B):\n",
    "    C = numpy.zeros((A.shape[0], B.shape[1]))\n",
    "    for i in xrange(A.shape[0]):\n",
    "        for j in xrange(B.shape[1]):\n",
    "            for k in xrange(A.shape[1]):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    return C\n",
    "\n",
    "\n",
    "m = 4\n",
    "n = 4\n",
    "p = 4\n",
    "A = numpy.random.uniform(size=(m, n))\n",
    "B = numpy.random.uniform(size=(n, p))\n",
    "C = numpy.random.uniform(size=(m, p))\n",
    "c = numpy.random.uniform()\n",
    "print numpy.allclose(matrix_matrix_product(A, B), numpy.dot(A, B))\n",
    "print numpy.allclose(matrix_matrix_product(A, (B + C)), matrix_matrix_product(A, B) + matrix_matrix_product(A, C))\n",
    "print numpy.allclose(matrix_matrix_product(A, c * B), c*matrix_matrix_product(A, B))\n",
    "print numpy.allclose(matrix_matrix_product(A, B), matrix_matrix_product(B, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Range and Null-Space\n",
    "\n",
    "The **range** of a matrix (similar to any function), denoted as $\\text{range}(A)$, is the set of vectors that can be expressed as $A x$ for some $x$.  We can also then say that that $\\text{range}(A)$ is the space **spanned** by the columns of $A$.  In other words the columns of $A$ provide a basis for $\\text{range}(A)$, also called the **column space** of the matrix $A$.  Similarly the **null-space** of a matrix $A$, denoted $\\text{null}(A)$ is the set of vectors $x$ that satisfy $A x = 0$.\n",
    "\n",
    "A similar concept is the **rank** of the matrix $A$, denoted as $\\text{rank}(A)$, is the dimension of the column space.  A matrix $A$ is said to have **full-rank** if $\\text{rank}(A) = \\min(m, n)$.  This property also implies that the matrix mapping is **one-to-one**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inverse\n",
    "\n",
    "A **non-singular** or **invertible** matrix is characterized as a matrix with full-rank.  This is related to why we know that the matrix is one-to-one, we can use it to transform a vector $x$ and using the inverse, denoted $A^{-1}$, we can map it back to the original matrix.  The familiar definition of this is\n",
    "\\begin{align*}\n",
    "    A x &= b \\\\\n",
    "    A^{-1} A x & = A^{-1} b \\\\\n",
    "    x &=A^{-1} b\n",
    "\\end{align*}\n",
    "where here $b$ is in the column space of $A$.\n",
    "\n",
    "There are a number of important properties of a non-singular matrix A.  Here we list them as the following equivalent statements\n",
    "1. $A$ has an inverse $A^{-1}$\n",
    "1. $\\text{rank}(A) = m$\n",
    "1. $\\text{range}(A) = \\mathbb{C}^m$\n",
    "1. $\\text{null}(A) = {0}$\n",
    "1. 0 is not an eigenvalue of $A$\n",
    "1. $\\text{det}(A) \\neq 0$\n",
    "\n",
    "#### Example:  Properties of invertible matrices\n",
    "\n",
    "Show that given an invertible matrix that the rest of the properties hold.  Make sure to search the `numpy` packages for relevant functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+00   0.00000000e+00  -1.11022302e-16]\n",
      " [ -4.44089210e-16   1.00000000e+00   0.00000000e+00]\n",
      " [  2.22044605e-16   2.22044605e-16   1.00000000e+00]]\n",
      "3\n",
      "range\n",
      "[ 0.  0.  0.]\n",
      "[ 1.54248933 -0.31991532  0.11452118]\n"
     ]
    }
   ],
   "source": [
    "m = 3\n",
    "for n in xrange(100):\n",
    "    A = numpy.random.uniform(size=(m, m))\n",
    "    if numpy.linalg.det(A) != 0:\n",
    "        break\n",
    "        \n",
    "print numpy.dot(numpy.linalg.inv(A), A)\n",
    "print numpy.linalg.matrix_rank(A)\n",
    "print \"range\"\n",
    "print numpy.linalg.solve(A, numpy.zeros(m))\n",
    "print numpy.linalg.eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Orthogonal Vectors and Matrices\n",
    "\n",
    "Orthogonality is a very important concept in linear algebra that forms the basis of many of the modern methods used in numerical computations.  Two vectors are said to be orthogonal if their **inner-product** or **dot-product** defined as\n",
    "$$\n",
    "    < x, y > = (x, y) = x \\cdot y = \\sum^m_{i=1} x_i y_i\n",
    "$$\n",
    "where here we have shown the various notations you may run into (the inner-product is in-fact a general term for a similar operation for mathematical objects such as functions).  If $<x,y> = 0$ then we say $x$ and $y$ are orthogonal.  The reason we use this terminology is that the inner-product of two vectors can also be written in terms of the angle between them where\n",
    "$$\n",
    "    \\cos \\theta = \\frac{< x, y >}{||x||~||y||}\n",
    "$$\n",
    "where $||x||$ is the Euclidean norm of the vector $x$.  We can write this in terms of the inner-product as well as\n",
    "$$\n",
    "    ||x||^2 = < x, x > = \\sum^m_{i=1} |x_i|^2.\n",
    "$$\n",
    "The generalization of the inner-product to complex spaces is defined as\n",
    "$$\n",
    "    < x, y > = \\sum^m_{i=1} x_i^* y_i\n",
    "$$\n",
    "where $x_i^*$ is the complex-conjugate of the value $x_i$.\n",
    "\n",
    "Taking this idea one step further we can say a set of vectors $x \\in X$ are orthogonal to $y \\in Y$ if $\\forall x,y$ $< x, y > = 0$.  If $\\forall x,y$ $||x|| = 1$ and $||y|| = 1$ then they are also called orthonormal.\n",
    "\n",
    "Another concept that is related to orthogonality is linear-independence.  A set of vectors $x \\in X$ are **linearly independent** if $\\forall x \\in X$ that each $x$ cannot be written as a linear combination of the other vectors in the set $X$.  An equivalent statement is that there does not exist a set of scalars $c_i$ such that\n",
    "$$\n",
    "    x_k = \\sum^n_{i=1, i \\neq k} c_i x_i\n",
    "$$\n",
    "is true.  Another way to write this is that $x_k$ is orthogonal to all the rest of the vectors in the set $X$.  This can be related directly through the idea of projection.  If we have a set of vectors $x \\in X$ we can project another vector $v$ onto the vectors in $X$ by using the inner-product.  This is especially powerful if we have a set of linearly-independent vectors $X$, which are said to **span** a space (or provide a **basis** for a space), s.t. any vector in the space spanned by $X$ can be expressed as a linear combination of the basis vectors $X$\n",
    "$$\n",
    "    v = \\sum^n_{i=1} < v, x_i > x_i.\n",
    "$$\n",
    "\n",
    "Looping finally back to matrices, the column space of a matrix is spanned by its linearly independent columns.  Any vector $v$ in the column space can therefore be expressed via the equation above.  A special class of matrices called **unitary** matrices when complex-valued and **orthogonal** when purely real-valued if the columns of the matrix are orthonormal to each other.  Importantly this implies that for a unitary matrix $Q$ we know the following\n",
    "\n",
    "1. $Q^* = Q^{-1}$\n",
    "1. $Q^*Q = I$\n",
    "\n",
    "where $Q^*$ is the **adjoint** of $Q$ defined as the transpose of the original matrix with the entries being the complex conjugate of each entry.  As an example if we have the matrix\n",
    "\\begin{align*}\n",
    "    Q &= \\begin{bmatrix} q_{11} & q_{12} \\\\ q_{21} & q_{22} \\\\ q_{31} & q_{32} \\end{bmatrix} ~~~~ \\text{then} \\\\\n",
    "    Q^* &= \\begin{bmatrix} q^*_{11} & q^*_{21} & q^*_{31} \\\\ q^*_{12} & q^*_{22} & q^*_{32} \\end{bmatrix}\n",
    "\\end{align*}\n",
    "The important part of being an unitary matrix is that the projection onto the column space of the matrix $Q$ preserves geometry in an Euclidean sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector Norms\n",
    "\n",
    "Norms (and also measures but let's not get too deep into that) provide a means for measure the \"size\" or distance in a space.  In general a norm is a function, denoted by $||\\cdot||$, that maps $\\mathbb{C}^m \\rightarrow \\mathbb{R}$.  In other words we stick in a multi-valued object and get a single, real-valued number out the other end.  All norms satisfy the properties:\n",
    "\n",
    "1. $||x|| \\geq 0$, and $||x|| = 0$ only if $x = 0$\n",
    "1. $||x + y|| \\leq ||x|| + ||y||$ (triangle inequality)\n",
    "1. $||c x|| = |c| ||x||$ where $c \\mathbb{C}$\n",
    "\n",
    "There are a number of relevant norms that we can define beyond the Euclidean norm, also know as the 2-norm or $\\ell_2$ norm:\n",
    "\n",
    "1. $||x||_1 = \\sum^m_{i=1} |x_i|$, called the $\\ell_1$ norm\n",
    "1. $||x||_2 = \\left( \\sum^m_{i=1} |x_i|^2 \\right)^{1/2}$, called the $\\ell_2$ norm\n",
    "1. $||x||_p = \\left( \\sum^m_{i=1} |x_i|^p \\right)^{1/p}$, where $1 \\leq p < \\infty$, called the $\\ell_p$ norm\n",
    "1. $||x||_\\infty = \\max |x_i|$, $\\ell_\\infty$ norm\n",
    "1. $||x||_{W_p} = \\left( \\sum^m_{i=1} |w_i x_i|^p \\right)^{1/p}$, where $1 \\leq p < \\infty$, called the weighted $\\ell_p$ norm\n",
    "\n",
    "These are also related to other norms denoted by capital letters ($L_2$ for instance).  In this case we use the lower-case notation to denote finite or discrete versions of the infinite dimensional counterparts.\n",
    "\n",
    "#### Example:  Comparisons Between Norms\n",
    "\n",
    "Compute the norms given some vector $x$ and compare their values.  Verify the properties of the norm for one of the norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.53142054289 1.33137354063 1.07417485131 0.849380848354\n",
      "1.28388962363 1.79060127591\n",
      "0.0849380848354 0.0849380848354\n"
     ]
    }
   ],
   "source": [
    "m = 10\n",
    "p = 4\n",
    "x = numpy.random.uniform(size=m)\n",
    "\n",
    "ell_1 = 0.0\n",
    "for i in xrange(m):\n",
    "    ell_1 += numpy.abs(x[i])\n",
    "    \n",
    "ell_2 = 0.0\n",
    "for i in xrange(m):\n",
    "    ell_2 += numpy.abs(x[i])**2\n",
    "ell_2 = numpy.sqrt(ell_2)\n",
    "\n",
    "ell_p = 0.0\n",
    "for i in xrange(m):\n",
    "    ell_p += numpy.abs(x[i])**p\n",
    "ell_p = (ell_2)**(1.0 / p)\n",
    "\n",
    "ell_infty = numpy.max(numpy.abs(x))\n",
    "\n",
    "print ell_1, ell_2, ell_p, ell_infty\n",
    "\n",
    "y = numpy.random.uniform(size=m)\n",
    "print numpy.max(numpy.abs(x + y)), numpy.max(numpy.abs(x)) + numpy.max(numpy.abs(y))\n",
    "print numpy.max(numpy.abs(0.1 * x)), 0.1 * numpy.max(numpy.abs(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix Norms\n",
    "\n",
    "The most direct way to consider a matrix norm is those induced by a vector-norm.  Given a vector norm, we can define a matrix norm as the smallest number $C$ that satisfies the inequality\n",
    "$$\n",
    "    ||A x||_{m} \\leq C ||x||_{n}.\n",
    "$$\n",
    "or as the supremum of the ratios so that\n",
    "$$\n",
    "    C = \\sup_{x\\in\\mathbb{C}^n ~ x\\neq0} \\frac{||A x||_{m}}{||x||_n}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Noting that $||A x||$ lives in the column space and $||x||$ on the domain we can think of the matrix norm as the \"size\" of the matrix that maps the domain to the range.  Also noting that if $||x||_n = 1$ we also satisfy the condition we can write the induced matrix norm as\n",
    "$$\n",
    "    ||A||_{(m,n)} = \\sup_{x \\in \\mathbb{C}^n ~ ||x||_{n} = 1} ||A x||_{m}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: Induced Matrix Norms\n",
    "\n",
    "Consider the matrix\n",
    "$$\n",
    "    A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix}.\n",
    "$$\n",
    "Compute the induced-matrix norm of $A$ for the vector norms $\\ell_2$ and $\\ell_\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. For both of the requested norms the unit-length vectors $[1, 0]$ and $[0, 1]$ can be used to determine the bounds on the matrix norms in each case.  $||A||_2 = \\sup_{x \\ in \\mathbb{R}^n} \\left( ||A \\cdot [1, 0]^T||_2, ||A \\cdot [0, 1]^T||_2 \\right )$  computing each of the norms we have\n",
    "\\begin{align*}\n",
    "    \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} &= \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\n",
    "\\end{align*}\n",
    "which translates into the norms $||A \\cdot [1, 0]^T||_2 = 1$ and $||A \\cdot [0, 1]^T||_2 = 2 \\sqrt{2}$.  This implies that the $\\ell_2$ induced matrix norm of $A$ is at least $||A||_{2} = 2 \\sqrt{2}$.\n",
    "\n",
    "  Turns out that $||A||_2 = \\sqrt{\\rho(A^\\ast A)}$ where $\\rho(B)$ is the spectral radius defined as\n",
    "$$\n",
    "    \\rho(B) = \\max_{i} |\\lambda_i|,\n",
    "$$\n",
    "i.e. the maximum absolute value of the eigenvalues of $B$.  Computing the norm again here we find\n",
    "$$\n",
    "    A^\\ast A = \\begin{bmatrix} 1 & 0 \\\\ 2 & 2 \\end{bmatrix} \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 \\\\ 2 & 8 \\end{bmatrix}\n",
    "$$\n",
    "which has eigenvalues \n",
    "$$\n",
    "    \\lambda = \\frac{1}{2}\\left(9 \\pm \\sqrt{65}\\right )\n",
    "$$\n",
    "so $||A||_2 \\approx 2.9208096$.\n",
    "\n",
    "1. We can again bound $||A||_\\infty$ by looking at the unit vectors which give us the matrix lower bound of 2.  To compute it turns out $||A||_{\\infty} = \\max_{1 \\leq i \\leq m} ||a^\\ast_i||_1$ where $a^\\ast_i$ is the $i$th row of $A$.  This represents then the maximum of the row sums of $A$.  Therefore $||A||_\\infty = 3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: General  Norms of a Matrix\n",
    "\n",
    "Compute a bound on the induced norm of the $m \\times n$ dimensional matrix $A$ using $\\ell_1$ and $\\ell_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One of the most useful ways to think about matrix norms is as a transformation of a unit-ball to an ellipse.  Depending on the norm in question, the norm will be some combination of the resulting ellipse.  For the above cases we have some nice relations based on these ideas.\n",
    "\n",
    "1. $||A x||_1 = || \\sum^n_{j=1} a_j x_j ||_1 \\leq \\sum^n_{j=1} |x_j| ||a_j||_1 \\leq \\max_{1\\leq j\\leq n} ||a_j||_1$\n",
    "1. $||A x||_\\infty = || \\sum^n_{j=1} a_j x_j||_\\infty \\leq \\sum^n_{j=1} |x_j| ||a_j||_\\infty \\leq \\max_{1 \\leq i \\leq m} ||a_i||_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Cauchy-Schwarz and Hölder Inequalities\n",
    "\n",
    "Computing matrix norms where $p \\neq 1$ or $\\infty$ is more difficult unfortunately.  We fortunately have a couple of tools that can be useful.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Cauchy-Schwarz Inequality**:  For the special case where $p=q=2$, for any vectors $x$ and $y$\n",
    "$$\n",
    "    |x^*y| \\leq ||x||_2 ||y||_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Hölder's Inequality**:  Turns out this holds in general if given a $p$ and $q$ that satisfy $1/p + 1/q = 1$ with $1 \\leq p, q \\leq \\infty$\n",
    "$$\n",
    "    |x^*y| \\leq ||x||_p ||y||_q.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### General Matrix Norms (non-induced)\n",
    "\n",
    "In general matrix-norms have the following properties whether they are induced from a vector-norm or not:\n",
    "1. $||A|| \\geq 0$ and $||A|| = 0$ only if $A = 0$\n",
    "1. $||A + B|| \\leq ||A|| + ||B||$ (Triangle Inequality)\n",
    "1. $||c A|| = |c| ||A||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The most widely used matrix norm not induced by a vector norm is the **Frobenius norm** defined by\n",
    "$$\n",
    "    ||A||_F = \\left( \\sum^m_{i=1} \\sum^n_{j=1} |A_{ij}|^2 \\right)^{1/2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Invariance under unitary multiplication\n",
    "\n",
    "One important property of the matrix 2-norm (and Forbenius norm) is that multiplication by a unitary matrix does not change the product (kind of like multiplication by 1).  In general for any $A \\in \\mathbb{C}^{m\\times n}$ and unitary matrix $Q \\in \\mathbb{C}^{m \\times m}$ we have\n",
    "\\begin{align*}\n",
    "    ||Q A||_2 &= ||A||_2 \\\\ ||Q A||_F &= ||A||_F.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## QR Factorizations and Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projections\n",
    "\n",
    "A **projector** is a square matrix $P$ that satisfies\n",
    "$$\n",
    "    P^2 = P.\n",
    "$$\n",
    "A projector comes from the idea that we want to project a vector $v$ onto a lower dimensional subspace.  Of course if $v$ lies completely within this subspace, i.e. $v \\in \\text{range}(P)$ then $P v = v$.  This motivates the definition above.  Take for instance a vector $x \\notin \\text{range}(P)$ and project it onto the subspace $Px = v$.  If we apply the projection again to $v$ now we have\n",
    "\\begin{align*}\n",
    "    Px &= v \\\\\n",
    "    P^2 x & = Pv = v \\Rightarrow \\\\\n",
    "    P^2 = P.\n",
    "\\end{align*}\n",
    "It is also important to keep in mind the following, given again $x \\notin \\text{range}(P)$, if we look at the difference between the projection and the original vector $Px - x$ and apply the projection again we have\n",
    "$$\n",
    "    P(Px - x) = P^2 x - Px = 0\n",
    "$$\n",
    "which means the difference between the projected vector $Px = v$ lies in the null space of $P$, $v \\in \\text{null}(P)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complimentary Projectors\n",
    "\n",
    "A projector also has a compliment defined as $I - P$.  We can show that again this a projector as\n",
    "$$\n",
    "    (I - P)^2 = I - IP - IP + P^2 = I - 2 P + P^2 = I - 2P - P = I - P.\n",
    "$$\n",
    "\n",
    "It turns out that the compliment projects exactly onto $\\text{null}(P)$.  This can be shown by considering an $x \\in \\text{null}(P)$, then $P x = 0$ and therefore $(I - P) x = x$ implying that $x \\in \\text{range}(I - P)$.  We also know that $(I - P) x = x - Px \\in \\text{null}(P)$ as well.  This shows that the $\\text{range}(I - P) \\subseteq \\text{null}(P)$ and $\\text{range}(I - P) \\supseteq \\text{null}(P)$ implying that $\\text{range}(I - P) = \\text{null}(P)$ exactly.  \n",
    "\n",
    "This result provides an important property of a projector and its compliment, namely that they divide a space into two subspaces whose intersection is $\\text{range}(I - P) \\cap \\text{range}(P) = \\{0\\}$.  These two spaces are said to also be complimentary subspaces.\n",
    "\n",
    "Another important consequence of this is that for any $P \\in \\mathbb C^{m \\times m}$ we know that $P$ and its compliment split the space $\\mathbb C^m$ into two pieces, say $S_1$ and $S_2$ spanned by the vectors $s_1$ and $s_2$.  Any $v \\in \\mathbb C^m$ then can be expressed as a linear combination of $s_1$ and $s_2$ via the projectors $P$ and $I - P$.  In other words if we want to split the vector $v$ up into pieces, one part say $v_1$ in the subspace $S_1$ and $v_2$ in the subspace $S_2$, we simply need to form the appropriate projector $P$ so that\n",
    "\\begin{align*}\n",
    "    P v = v_1 ~~~~~~~ (I - P) v &= v_2 \\Rightarrow \\\\\n",
    "    v &= v_1 + v_2.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orthogonal Projectors\n",
    "\n",
    "An **orthogonal projector** is one that projects onto a subspace $S_1$ that is orthogonal the complimentary subspace $S_2$ (this is also phrased that $S_1$ projects along a space $S_2$).  Note that we are only talking about the subspaces (and their basis), not the projectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An orthogonal projector $P$ also turns out to be **hermitian**, a matrix that has the property that $P^* = P$.  There is fact a theorem that states:\n",
    "\n",
    "*A projector $P$ is orthogonal if and only if $P = P^\\ast$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection with an Orthonormal Basis\n",
    "\n",
    "We can also directly construct a projector that uses an orthonormal basis on the subspace $S_1$.  If we define another matrix $Q \\in \\mathbb C^{m \\times n}$ which is unitary (its columns are orthonormal) we can construct an orthogonal projector as\n",
    "$$\n",
    "    P = Q Q^*.\n",
    "$$\n",
    "Note that the resulting matrix $P \\in \\mathbb C^{m \\times m}$ as we require.  This means also that the dimension of the subspace $S_1$ is $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:  Construction of an orthonormal projector**\n",
    "\n",
    "Take $\\mathbb R^3$ and derive a projector that projects onto the x-y plane and is an orthogonal projector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "$$Q Q^\\ast = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}$$\n",
    "\n",
    "$$P = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Construction of a projector that eliminates a direction**\n",
    "\n",
    "Say we are in $\\mathbb C^m$ and we want to eliminate the component of a vector in one direction, say $q$.  We can do this by forming the projector that projects into that direction $P = q q^* \\in \\mathbb C^{m \\times m}$.  The compliment will then include everything BUT that direction so that $I - P = I - q q^*$ gives us what we want.  This assumes that $||q|| = 1$, if this is not the case we can also generalize this to an arbitrary vector $v$ so that the correct projector is then\n",
    "$$\n",
    "    I - \\frac{a a^\\ast}{||a||} = I - \\frac{a a^\\ast}{a^\\ast a}\n",
    "$$\n",
    "Note that differences in the resulting dimensions between the two values in the fraction.  Also note that as we saw with the outer product, the resulting $\\text{rank}(q q^\\ast) = 1$.\n",
    "\n",
    "Try to construct a projector in $\\mathbb R^3$ that projects onto the $x$-$y$ plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 5]\n",
      "[ 3.  4.  0.]\n",
      "[0 0 5]\n",
      "[ 3.  4.  0.]\n"
     ]
    }
   ],
   "source": [
    "q = numpy.array([0, 0, 1])\n",
    "P = numpy.outer(q, q.conjugate())\n",
    "P_comp = numpy.identity(3) - P\n",
    "\n",
    "x = numpy.array([3, 4, 5])\n",
    "print numpy.dot(P, x)\n",
    "print numpy.dot(P_comp, x)\n",
    "\n",
    "a = numpy.array([0, 0, 3])\n",
    "P = numpy.outer(a, a.conjugate()) / (numpy.dot(a, a.conjugate()))\n",
    "P_comp = numpy.identity(3) - P\n",
    "print numpy.dot(P, x)\n",
    "print numpy.dot(P_comp, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR Factorization\n",
    "\n",
    "One of the most important ideas in linear algebra is the concept of factorizing an original matrix into different constituents which may have various properties.  In numerical linear algebra one of the most important factorizations is the QR factorizations.  \n",
    "\n",
    "The basic idea is that we want to break up $A$ into its successive spaces spanned by the columns of $A$.  If we have\n",
    "$$\n",
    "    A = \\begin{bmatrix} ~ & ~ &  \\\\ ~ & ~ &  \\\\ a_1 & \\cdots & a_n \\\\ ~ & ~ &  \\\\ ~ & ~ & ~ \\end{bmatrix}\n",
    "$$\n",
    "then we want to construct the sequence\n",
    "$$\n",
    "    \\text{span}(a_1) \\subseteq \\text{span}(a_1, a_2) \\subseteq \\text{span}(a_1, a_2, a_3) \\subseteq \\cdots \\subseteq \\text{span}(a_1, a_2, \\ldots , a_n)\n",
    "$$\n",
    "where here $\\text{span}(v_i)$ indicates the subspace spanned by the vectors $v_i$.  QR factorization attempts to construct a set of orthonormal vectors $q_i$ that span each of the subspaces, i.e. \n",
    "$$\n",
    "    \\text{span}(a_1, a_2, \\ldots, a_j) = \\text{span}(q_1, q_2, \\ldots, q_j).\n",
    "$$\n",
    "\n",
    "Lets consider this for the first few vectors $q_i$.\n",
    "1. For $\\text{span}(a_1)$ we can directly use $a_1$ but normalize the vector such that\n",
    "$$\n",
    "    q_1 = \\frac{a_1}{||a_1||}.\n",
    "$$\n",
    "2. For $\\text{span}(a_1, a_2)$ we already have $q_1$ so we need to have a vector $q_2$ that is orthogonal to $q_1$, i.e.\n",
    "$$\n",
    "    \\langle q_1, q_2 \\rangle = q_1^* q_2 = 0\n",
    "$$\n",
    "and is again normalized.  We can accomplish this by modifying $a_2$ such that\n",
    "$$\n",
    "    q_2 = \\frac{a_2 - \\langle q_1, a_2\\rangle}{||a_2 - \\langle q_1, a_2\\rangle||}.\n",
    "$$\n",
    "which we can show is orthogonal to $q_1$ as\n",
    "\\begin{align*}\n",
    "    \\langle q_1, q_2 \\rangle &= \\left(\\langle  q_1, a_2 - \\langle  q_1, a_2 \\rangle \\rangle \\right) \\frac{1}{||a_2 - \\langle  q_1, a_2 \\rangle||} \\\\\n",
    "    &= \\left(\\langle  q_1, a_2 \\rangle - \\langle  q_1, a_2 \\rangle\\right) \\frac{1}{||a_2 - \\langle  q_1, a_2 \\rangle||} \\\\\n",
    "    &= 0.\n",
    "\\end{align*}\n",
    "\n",
    "These results suggest then that we may have a matrix factorization that has the following form:\n",
    "$$\n",
    "    \\begin{bmatrix} ~ & ~ &  \\\\ ~ & ~ &  \\\\ a_1 & \\cdots & a_n \\\\ ~ & ~ &  \\\\ ~ & ~ & ~ \\end{bmatrix} = \n",
    "    \\begin{bmatrix} ~ & ~ &  \\\\ ~ & ~ &  \\\\ q_1 & \\cdots & q_n \\\\ ~ & ~ &  \\\\ ~ & ~ & ~ \\end{bmatrix}\n",
    "    \\begin{bmatrix} r_{11} & r_{12} & \\cdots & r_{1n} \\\\ ~ & r_{22} & ~ & ~ \\\\ ~ & ~ & \\ddots & \\vdots \\\\ ~ & ~ & ~ & r_{nn} \\end{bmatrix}.\n",
    "$$\n",
    "If write out as a matrix multiplication we have\n",
    "\\begin{align*}\n",
    "    a_1 &= r_{11} q_1 \\\\\n",
    "    a_2 &= r_{22} q_2 + r_{12} q_1 \\\\\n",
    "    a_3 &= r_{33} q_3 + r_{23} q_2 + r_{13} q_1 \\\\\n",
    "    &\\vdots\n",
    "\\end{align*}\n",
    "we can also identify at least the first couple of value of $r$ as\n",
    "\\begin{align*}\n",
    "    r_{11} &= ||a_1|| \\\\\n",
    "    r_{12} &= \\langle q_1, a_2 \\rangle \\\\\n",
    "    r_{22} &= ||a_2 - r_{12} q_1||.\n",
    "\\end{align*}\n",
    "It turns out we can generalize this as Gram-Schmidt orthogonalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gram-Schmidt Orthogonalization\n",
    "\n",
    "As may have been suggestive we can directly construct the arrays $Q$ and $R$ via a process of successive orthogonalization.  We have already shown the first two iterations so lets not consider the $j$th iteration.  We want to subtract off the components of the vector $a_j$ in the direction of the $q_i$ vectors where $i=1,\\ldots,j-1$.  This suggests that we define a vector $v_j$ such that\n",
    "\\begin{align*}\n",
    "    v_j &= a_j - \\langle q_1, a_j \\rangle - \\langle q_2, a_j \\rangle - \\cdots - \\langle q_{j-1}, a_j \\rangle \\\\\n",
    "    &= a_j - \\sum^{j-1}_{i=1} \\langle q_i, a_j \\rangle.\n",
    "\\end{align*}\n",
    "We also need to normalize $v_j$ which allows to define the $j$th column of $Q$ as\n",
    "\\[\n",
    "    q_j = \\frac{a_j - \\sum^{j-1}_{i=1} \\langle q_i, a_j \\rangle}{||a_j - \\sum^{j-1}_{i=1} \\langle q_i, a_j \\rangle||}.\n",
    "\\]\n",
    "We can also therefore discern what the entries of $R$ are as we can write the matrix multiplication as the sequence\n",
    "\\begin{align*}\n",
    "    q_1 &= \\frac{a_1}{r_{11}} \\\\\n",
    "    q_2 &= \\frac{a_2 - r_{12} q_1}{r_{22}} \\\\\n",
    "    q_3 &= \\frac{a_3 - r_{13} q_1 - r_{23} q_2}{r_{33}} \\\\\n",
    "    &\\vdots \\\\\n",
    "    q_n &= \\frac{a_n - \\sum^{n-1}_{i=1} r_{in} q_i}{r_{nn}}\n",
    "\\end{align*}\n",
    "leading us to define\n",
    "\\begin{equation*}\n",
    "    r_{ij} = \\left \\{ \\begin{aligned}\n",
    "        &\\langle q_i, a_j \\rangle & &i \\neq j \\\\\n",
    "        &||a_j - \\sum^{j-1}_{i=1} r_{ij} q_i || & &i = j\n",
    "    \\end{aligned} \\right .\n",
    "\\end{equation*}\n",
    "\n",
    "This is called the **classical Gram-Schmidt** iteration.  Turns out that the procedure above is unstable because of rounding errors introduced.\n",
    "\n",
    "#### Full vs. Reduced QR\n",
    "One other important distinction for the QR factorization is the **full QR factorization** versus the **reduced QR factorization** which we have been investigating above.  We can extend the matrix $Q$ so that $Q \\in \\mathbb C^{m\\times m}$ and unitary requiring appending rows of zeros onto $R$ such that $R \\in \\mathbb C^{m \\times n}$.  The important distinction between the two is that the additional columns of $Q$ that we have added actually provide an orthonormal basis that is orthogonal to $\\text{range}(A)$.  In the case where $A$ is full ranked then they provide a basis for $\\text{null}(A^\\ast)$.\n",
    "\n",
    "#### QR Existence and Uniqueness\n",
    "Two important theorems exist regarding this algorithm which we state without proof:\n",
    "\n",
    "*Every $A \\in \\mathbb C^{m \\times n}$ with $m \\geq n$ has a full QR factorization and therefore a reduced QR factorization.*\n",
    "\n",
    "*Each $A \\in \\mathbb C^{m \\times n}$ with $m \\geq n$ of full rank has a unique reduced QR factorization $A = QR$ with $r_{jj} > 0$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08200424 -0.08595048  0.21821317]\n",
      " [-0.02616346 -1.14909182  0.42199391]\n",
      " [-0.62607364  0.34554027 -0.31836604]\n",
      " [ 1.35333876  1.1209395   0.39743517]]\n",
      "[[ 0.05490299 -0.05227196  0.39217267]\n",
      " [-0.0175168  -0.69883596  0.88501194]\n",
      " [-0.41916504  0.21014506 -0.22511886]\n",
      " [ 0.90607919  0.68171474 -0.11079697]]\n",
      "[[ 1.49362085  0.          0.49814422]\n",
      " [ 0.          1.64429406  0.        ]\n",
      " [ 0.          0.          0.48668247]]\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [ -3.46944695e-18   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Implement Classical Gram-Schmidt Iteration\n",
    "\n",
    "def classic_GS(A):\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    Q = numpy.empty((m, n))\n",
    "    R = numpy.zeros((n, n))\n",
    "    for j in xrange(n):\n",
    "        v = A[:, j]\n",
    "        for i in xrange(j - 1):\n",
    "            R[i, j] = numpy.dot(Q[:, i].conjugate(), A[:, j])\n",
    "            v = v - R[i, j] * Q[:, i]\n",
    "        R[j, j] = numpy.linalg.norm(v, ord=2)\n",
    "        Q[:, j] = v / R[j, j]\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "m = 4\n",
    "n = 3\n",
    "A = numpy.random.normal(size=(m, n))\n",
    "Q, R = classic_GS(A)\n",
    "print A\n",
    "print Q\n",
    "print R\n",
    "print numpy.dot(Q, R) - A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving $Ax = b$ with QR\n",
    "\n",
    "Suppose we want to solve the system $Ax = b$ where $A \\in \\mathbb C^{m \\times m}$.  We can use $QR$ factorization to do this!  Say we have found the QR factorization of $A$, then\n",
    "\\begin{align*}\n",
    "    A x &= b \\\\\n",
    "    QR x & = b \\\\\n",
    "    Q^\\ast Q R x &= Q^\\ast b \\\\\n",
    "    R x &= Q^\\ast b.\n",
    "\\end{align*}\n",
    "\n",
    "How has this helped us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gram-Schmidt in Terms of Projections\n",
    "\n",
    "To start lets rewrite classical Gram-Schmidt as a series of projections:\n",
    "$$\n",
    "    q_1 = \\frac{P_1 a_1}{||P_1 a_1||}, ~~~~~ q_2 = \\frac{P_2 a_2}{||P_2 a_2||}, ~~~~~ \\cdots ~~~~~ q_n = \\frac{P_n a_n}{||P_n a_n||}\n",
    "$$\n",
    "where the $P_i$ are orthogonal projectors onto the $q_1, q_2, \\ldots, q_{i-1}$, in other words the compliment of $\\text{span}(a_1, a_2, \\ldots, a_{i-1})$.\n",
    "\n",
    "How should we construct these projectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw before that we can easily construct an orthogonal projector onto the compliment of space by first constructing the projector onto the space itself via\n",
    "$$\n",
    "    \\hat{Q}_{i-1} = \\begin{bmatrix}\n",
    "        ~   & ~   & ~      & ~       \\\\\n",
    "        ~   & ~   & ~      & ~       \\\\\n",
    "        q_1 & q_2 & \\cdots & q_{i-1} \\\\ \n",
    "        ~   & ~   & ~      & ~       \\\\\n",
    "        ~   & ~   & ~      & ~      \n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "Constructing the projection onto the space spanned by $q_1$ through $q_{i-1}$ is then\n",
    "$$\n",
    "    \\hat{P}_{i-1} = \\hat{Q}_{i-1} \\hat{Q}^\\ast_{i-1}\n",
    "$$\n",
    "and therefore the projections in Gram-Schmidt orthogonalization is\n",
    "$$\n",
    "    P_{i} = I - \\hat{P}_{i-1} = I - \\hat{Q}_{i-1} \\hat{Q}^\\ast_{i-1}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified Gram-Schmidt\n",
    "\n",
    "We know will derive a modified version of the Gram-Schmidt algorithm that is more numerically stable.  Recall that the basic piece of the original algorithm was to take the inner product of $a_j$ and all the relevant $q_i$.  Using the rewritten version of Gram-Schmidt in terms of projections we then have\n",
    "$$\n",
    "    v_i = P_i a_i.\n",
    "$$\n",
    "This projection is of rank $m - (i - 1)$ as we know that the resulting $v_i$ are linearly independent by construction.  The modified version of Gram-Schmidt instead uses projections that are all of rank $m-1$.  To construct this projection remember that we can again construct the complement to a projection and perform the following sequence of projections\n",
    "$$\n",
    "    P_i = \\hat{P}_{q_{i-1}} \\hat{P}_{q_{i-2}} \\cdots \\hat{P}_{q_{2}} \\hat{P}_{q_{1}}\n",
    "$$\n",
    "where $\\hat{P}_{q_{i}}$ projects onto the complement of the space spanned by $q_i$.  Note that this performs mathematically the same job as $P_i a_i$ however each of these projectors are of rank $m - 1$.  This leads to the following set of calculations:\n",
    "\\begin{align*}\n",
    "    1.~~ v^{(1)}_i &= a_i  \\\\\n",
    "    2.~~ v^{(2)}_i &= \\hat{P}_{q_1} v_i^{(1)} = v^{(1)}_i - q_1 q_1^\\ast v^{(1)}_i \\\\\n",
    "    3.~~ v^{(3)}_i &= \\hat{P}_{q_2} v_i^{(2)} = v^{(2)}_i - q_2 q_2^\\ast v^{(2)}_i \\\\\n",
    "    & ~~ \\vdots &~&\\\\\n",
    "    i.~~ v^{(i)}_i &= \\hat{P}_{q_{i-1}} v_i^{(i-1)} =  v_i^{(i-1)} - q_{i-1} q_{i-1}^\\ast v^{(i-1)}_i\n",
    "\\end{align*}\n",
    "The reason why this approach is more stable is that we are not projecting with a possibly arbitrarily low-rank projector, instead we only take projectors that are high-rank.\n",
    "\n",
    "**Example: Implementation of modified Gram-Schmidt**\n",
    "Implement the modified Gram-Schmidt algorithm checking to make sure the resulting factorization has the required properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.97369944  1.5486792   0.71487063  0.02811839]\n",
      " [-1.90002601  0.10640225  0.28885925 -0.0253138 ]\n",
      " [-1.8140598  -0.3539455  -0.0271379   0.36413643]\n",
      " [ 2.89425075 -0.23573549  0.69921142 -1.13425054]]\n",
      "[[ 4.02811243 -0.43452472  0.20555874 -0.97381907]\n",
      " [ 0.          1.54976309  0.69167817 -0.15731236]\n",
      " [ 0.          0.          0.75062618 -0.64104798]\n",
      " [ 0.          0.          0.          0.19114861]]\n",
      "Modified = \n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00  -6.93889390e-18]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.38777878e-17]\n",
      " [  0.00000000e+00   0.00000000e+00  -3.46944695e-18   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00  -2.22044605e-16]]\n"
     ]
    }
   ],
   "source": [
    "# Implement Modified Gram-Schmidt Iteration\n",
    "def mod_GS(A):\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    Q = numpy.empty((m, n))\n",
    "    R = numpy.zeros((n, n))\n",
    "    v = A.copy()\n",
    "    for i in xrange(n):\n",
    "        R[i, i] = numpy.linalg.norm(v[:, i], ord=2)\n",
    "        Q[:, i] = v[:, i] / R[i, i]\n",
    "        for j in xrange(i + 1, n):\n",
    "            R[i, j] = numpy.dot(Q[:, i].conjugate(), v[:, j])\n",
    "            v[:, j] -= R[i, j] * Q[:, i]\n",
    "    return Q, R\n",
    "\n",
    "\n",
    "m = 4\n",
    "n = 4\n",
    "A = numpy.random.normal(size=(m, n))\n",
    "print A\n",
    "Q_mod, R_mod = mod_GS(A)\n",
    "print R_mod\n",
    "print \"Modified = \"\n",
    "print numpy.dot(Q_mod, R_mod) - A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Householder Triangularization\n",
    "\n",
    "One way to also interpret Gram-Schmidt orthogonalization is as a series of multiplications by upper triangular matrices of the matrix A.  For instance the first step in performing the first step in the modified algorithm is to divide through by the norm $r_{11} = ||v_1||$ to give $q_1$:\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        ~   & ~   & ~      & ~ \\\\\n",
    "        ~   & ~   & ~      & ~ \\\\\n",
    "        v_1 & v_2 & \\cdots & v_n \\\\\n",
    "        ~   & ~   & ~      & ~ \\\\\n",
    "        ~   & ~   & ~      & ~      \n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\frac{1}{r_{11}}   & 0 &\\cdots & ~      \\\\\n",
    "        ~   & 1   & ~         \\\\\n",
    "        ~ & ~ & \\ddots & ~ \\\\ \n",
    "        ~   & ~   & ~ & 1 \n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        ~   & ~   & ~      & ~       \\\\\n",
    "        ~   & ~   & ~      & ~       \\\\\n",
    "        q_1 & v_2 & \\cdots & v_n \\\\ \n",
    "        ~   & ~   & ~      & ~       \\\\\n",
    "        ~   & ~   & ~      & ~      \n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "We can also perform all the step (2) evaluations by also combining the step that projects onto the complement of $q_1$ by add the appropriate values to the entire first row:\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        ~   & ~   & ~      & ~ \\\\\n",
    "        ~   & ~   & ~      & ~ \\\\\n",
    "        v_1 & v_2 & \\cdots & v_n \\\\\n",
    "        ~   & ~   & ~      & ~ \\\\\n",
    "        ~   & ~   & ~      & ~      \n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\frac{1}{r_{11}}   & -\\frac{r_{12}}{r_{11}} & -\\frac{r_{13}}{r_{11}} & \\cdots      \\\\\n",
    "        ~   & 1   & ~         \\\\\n",
    "        ~ & ~ & \\ddots & ~ \\\\ \n",
    "        ~   & ~   & ~ & 1 \n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        ~   & ~   & ~      & ~       \\\\\n",
    "        ~   & ~   & ~      & ~       \\\\\n",
    "        q_1 & v_2^{(2)} & \\cdots & v_n^{(2)} \\\\ \n",
    "        ~   & ~   & ~      & ~       \\\\\n",
    "        ~   & ~   & ~      & ~      \n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "The next step can then be placed into the second row:\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        ~   & ~   & ~      & ~ \\\\\n",
    "        ~   & ~   & ~      & ~ \\\\\n",
    "        v_1 & v_2 & \\cdots & v_n \\\\\n",
    "        ~   & ~   & ~      & ~ \\\\\n",
    "        ~   & ~   & ~      & ~      \n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        1 & ~ & ~ & ~ & ~\\\\\n",
    "        ~ & \\frac{1}{r_{22}}   & -\\frac{r_{23}}{r_{22}} & -\\frac{r_{25}}{r_{22}} & \\cdots      \\\\\n",
    "        ~ & ~  & 1   & ~         \\\\\n",
    "        ~ & ~ & ~ & \\ddots & ~ \\\\ \n",
    "        ~   & ~   & ~ & ~& 1 \n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        ~   & ~   & ~      & ~       \\\\\n",
    "        ~   & ~   & ~      & ~       \\\\\n",
    "        q_1 & q_2 & \\cdots & v_n^{(3)} \\\\ \n",
    "        ~   & ~   & ~      & ~       \\\\\n",
    "        ~   & ~   & ~      & ~      \n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "If we identify the matrices as $R_1$ for the first case, $R_2$ for the second case and so on we can write the algorithm as\n",
    "$$\n",
    "    A \\underbrace{R_1R_2 \\cdots R_n}_{\\hat{R}^{-1}} = \\hat{Q}.\n",
    "$$\n",
    "This view of Gram-Schmidt is called Gram-Schmidt triangularization.\n",
    "\n",
    "Householder triangularization is similar in spirit.  Instead of multipling $A$ on the right Householder multiplies $A$ on the left by unitary matrices $Q_k$.  Remember that a unitary matrix (or an orthogonal matrix when strictly real) has as it's inverse it's adjoint (transpose when real) $Q^* = Q^{-1}$ so that $Q^* Q = I$.  We therefore have\n",
    "$$\n",
    "    Q_n Q_{n-1} \\cdots Q_2 Q_1 A = R\n",
    "$$\n",
    "which if we identify $Q_n Q_{n-1} \\cdots Q_2 Q_1 = Q^*$ and note that if $Q = Q^\\ast_n Q^\\ast_{n-1} \\cdots Q^\\ast_2 Q^\\ast_1$ then $Q$ is also unitary.  This then allows us to write\n",
    "\\begin{align*}\n",
    "    Q_n Q_{n-1} \\cdots Q_2 Q_1 A &= R \\\\\n",
    "    Q_{n-1} \\cdots Q_2 Q_1 A &= Q^\\ast_n R \\\\\n",
    "    &~~ \\vdots \\\\\n",
    "    A &= Q^\\ast_1 Q^\\ast_2 \\cdots Q^\\ast_{n-1} Q^\\ast_n R \\\\\n",
    "    A &= Q R\n",
    "\\end{align*}\n",
    "This was we can think of Householder triangularization as one of introducing zeros into $A$ via orthogonal matrices.\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        \\text{x} & \\text{x} & \\text{x} \\\\\n",
    "        \\text{x} & \\text{x} & \\text{x} \\\\\n",
    "        \\text{x} & \\text{x} & \\text{x} \\\\\n",
    "        \\text{x} & \\text{x} & \\text{x} \\\\\n",
    "    \\end{bmatrix} \\overset{Q_1}{\\rightarrow}\n",
    "    \\begin{bmatrix}\n",
    "        \\text{x} & \\text{x} & \\text{x} \\\\\n",
    "        0 & \\text{x} & \\text{x} \\\\\n",
    "        0 & \\text{x} & \\text{x} \\\\\n",
    "        0 & \\text{x} & \\text{x} \\\\\n",
    "    \\end{bmatrix} \\overset{Q_2}{\\rightarrow}\n",
    "    \\begin{bmatrix}\n",
    "        \\text{x} & \\text{x} & \\text{x} \\\\\n",
    "        0 & \\text{x} & \\text{x} \\\\\n",
    "        0 & 0 & \\text{x} \\\\\n",
    "        0 & 0 & \\text{x} \\\\\n",
    "    \\end{bmatrix} \\overset{Q_3}{\\rightarrow}\n",
    "    \\begin{bmatrix}\n",
    "        \\text{x} & \\text{x} & \\text{x} \\\\\n",
    "        0 & \\text{x} & \\text{x} \\\\\n",
    "        0 & 0 & \\text{x} \\\\\n",
    "        0 & 0 & 0\\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now the question is how do we contruct the $Q_k$.  The construction is usually broken down into a matrix of the form\n",
    "$$\n",
    "    Q_k = \\begin{bmatrix} I & 0 \\\\ 0 & F \\end{bmatrix}\n",
    "$$\n",
    "where $I \\in \\mathbb C^{k-1 \\times k-1}$ identity matrix and $F \\mathbb C^{m - (k - 1) \\times m - (k-1)}$ unitary matrix.  Note that this will leave the rows and columns we have already worked on alone and be unitary.  To construct $F$ consider the transformation that reflects the vector $x$ over the plane $H$ so that $F x = v = ||x|| e_1$:\n",
    "![Householder reflection](./images/householder.png)\n",
    "or mathematically\n",
    "$$\n",
    "    x = \\begin{bmatrix}\n",
    "        \\text{x} \\\\\n",
    "        \\text{x} \\\\\n",
    "        \\text{x} \\\\\n",
    "        \\text{x}\n",
    "    \\end{bmatrix} \\overset{F}{\\rightarrow}\n",
    "    Fx = \\begin{bmatrix}\n",
    "        ||x|| \\\\\n",
    "        0 \\\\\n",
    "        0 \\\\\n",
    "        0\n",
    "    \\end{bmatrix} = ||x|| e_1.\n",
    "$$\n",
    "This is of course the effect on only one vector.  Any other vector will be reflected across $H$ (technically a hyperplane) which is orthogonal to $v = ||x|| e_1 - x$.  This has a similar construction as to the projector compliments we were working with before.  Consider the projector defined as\n",
    "$$\n",
    "    P x = \\left (I - \\frac{v v^\\ast}{v^\\ast v}\\right) x = x - x \\left(\\frac{v^\\ast x}{v^\\ast v} \\right),\n",
    "$$\n",
    "the compliment of a projection in the direction of the vector $v$, in other words in the direction of $H$ above.  Since we actually want to transform $x$ to lie in the direction of $e_1$ we need to go twice as far as just the projection onto $H$.  This allows us to identify the matrix $F$ as\n",
    "$$\n",
    "    F = I - 2 \\frac{v v^\\ast}{v^\\ast v}.\n",
    "$$\n",
    "\n",
    "There is actually a non-uniqueness to which direction we reflect over since another definition of $\\hat{H}$ which is orthogonal to the one we originally choose is available.  For numerical stability purposes we will choose the reflector that is the most different from $x$.  This comes back to having difficulties numerically when the vector $x$ is nearly aligned with $e_1$ and therefore one of the $H$ specification.  By convention the $v$ chosen is defined by\n",
    "$$\n",
    "    v = \\text{sign}(x_1)||x|| e_1 + x.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08682739  0.32333397  1.21206833]\n",
      " [-0.41406249  0.51045844 -0.3580182 ]\n",
      " [ 1.63615889 -0.32497515 -1.17774042]\n",
      " [-1.71520245 -1.63936116  0.05935805]]\n",
      "step 1: R = \n",
      "[[ -2.40788747e+00  -8.70820531e-01   7.37284823e-01]\n",
      " [ -5.55111512e-17   7.08659283e-01  -2.79215593e-01]\n",
      " [  0.00000000e+00  -1.10816145e+00  -1.48912721e+00]\n",
      " [  0.00000000e+00  -8.18338779e-01   3.85788072e-01]]\n",
      "step 2: R = \n",
      "[[ -2.40788747e+00  -8.70820531e-01   7.37284823e-01]\n",
      " [ -5.55111512e-17  -1.54916046e+00  -7.33700179e-01]\n",
      " [  0.00000000e+00   2.22044605e-16  -1.26606144e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   5.50514402e-01]]\n",
      "step 3: R = \n",
      "[[ -2.40788747e+00  -8.70820531e-01   7.37284823e-01]\n",
      " [ -5.55111512e-17  -1.54916046e+00  -7.33700179e-01]\n",
      " [  0.00000000e+00   2.22044605e-16   1.38057151e+00]\n",
      " [  0.00000000e+00   0.00000000e+00  -1.11022302e-16]]\n",
      "Householder = \n",
      "[[ -2.40788747e+00  -8.70820531e-01   7.37284823e-01]\n",
      " [ -5.55111512e-17  -1.54916046e+00  -7.33700179e-01]\n",
      " [  0.00000000e+00   2.22044605e-16   1.38057151e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Implementation of Householder QR Factorization\n",
    "def householder_QR(A, verbose=False):\n",
    "    R = A.copy()\n",
    "    m, n = A.shape\n",
    "    v = numpy.empty((m, n))\n",
    "    for k in xrange(n):\n",
    "        x = R[k:, k]\n",
    "        e1 = numpy.zeros(x.shape)\n",
    "        e1[0] = 1.0\n",
    "        v[k:, k] = numpy.sign(x[0]) * numpy.linalg.norm(x, ord=2) * e1 + x\n",
    "        v[k:, k] = v[k:, k] / numpy.linalg.norm(v[k:, k], ord=2)\n",
    "        R[k:, k:] -= 2.0 * numpy.dot(numpy.outer(v[k:, k], v[k:, k].conjugate()), R[k:, k:])\n",
    "        if verbose:\n",
    "            print \"step %s: R = \" % (k+1)\n",
    "            print R\n",
    "    \n",
    "    return R[:n, :n], v\n",
    "\n",
    "m = 4\n",
    "n = 3\n",
    "A = numpy.random.normal(size=(m, n))\n",
    "print A\n",
    "R_hh, v = householder_QR(A, verbose=True)\n",
    "print \"Householder = \"\n",
    "print R_hh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above algorithm we do not explicitly form the matrix $Q$ to save memory and computation.  If we wanted to for instance solve $A x = b$ we again have\n",
    "\\begin{align*}\n",
    "    A x &= b \\\\\n",
    "    Q R x &= b \\\\\n",
    "    Rx &= Q^\\ast b.\n",
    "\\end{align*}\n",
    "This requires then the multiplication $Q^\\ast b$.  To do this we just need to recognize that the vectors $v$ we saves specify the $Q_k$ so we can form the matrices $F$ and multiply the vector directly with the $v_k$s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.18584044 -0.13874776  1.19767718  0.61708215]\n"
     ]
    }
   ],
   "source": [
    "m = 4\n",
    "n = 3\n",
    "A = numpy.random.normal(size=(m, n))\n",
    "b = numpy.random.normal(size=m)\n",
    "R_hh, v = householder_QR(A)\n",
    "for k in xrange(n):\n",
    "    b[k:] -= 2.0 * numpy.dot(numpy.outer(v[k:, k], v[k:, k].conjugate()), b[k:])\n",
    "print b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly if we want to compute a matrix vector product of $Qx$ we can use the following algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.18584044 -0.13874776  1.19767718  0.61708215]\n"
     ]
    }
   ],
   "source": [
    "m = 4\n",
    "n = 3\n",
    "A = numpy.random.normal(size=(m, n))\n",
    "x = numpy.random.normal(size=m)\n",
    "R_hh, v = householder_QR(A)\n",
    "for k in xrange(n-1, 0, -1):\n",
    "    x[k:] -= 2.0 * numpy.dot(numpy.outer(v[k:, k], v[k:, k].conjugate()), x[k:])\n",
    "print b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares Problems\n",
    "\n",
    "Least squares problems have already been introduced but lets consider how our QR factorizations might help us.  As before the least squares problem is characterized by wanting to find the $b$ such that $||b - Ax||_2$ is minimized over $x \\in \\mathbb C^n$.  Since we are using the $\\ell_2$ norm is equivalent to the Euclidean norm we know that there is a geometric interpretation to this goal, find the vector $x$ that gives the minimum distance between the vector $b$ and $A x$.  This can be interpreted as a projection:\n",
    "![Least-Squares Projection](./images/lsq_projection.png)\n",
    "where\n",
    "$$\n",
    "    r = b - Ax\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    y = Ax = Pb.\n",
    "$$\n",
    "The vector $r$ is called the residual (and the thing we are trying to minimize).  $P$ represents the orthogonal projector onto the $\\text{range}(A)$.\n",
    "\n",
    "QR factorization plays a role similar to the ideas we saw from Householder triangularization.  Define the orthogonal projector $P = Q Q^\\ast$ based on the reduced QR factorization of $A$.  We know then that the projector projects onto the span of column space of $A$ ($\\text{span}(A)$).  Using this QR factorization we know that the least-squares formulation then becomes\n",
    "\\begin{align*}\n",
    "    A^\\ast A x &= A^\\ast b \\\\\n",
    "    R^\\ast Q^\\ast Q R x &= R^\\ast Q^\\ast b \\\\\n",
    "    R^\\ast R x & = R^\\ast Q^\\ast b \\\\\n",
    "    R x & = Q^\\ast b\n",
    "\\end{align*}\n",
    "reducing the least-squares calculation to one of finding the QR factorization and backwards substitution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditioning and Stability\n",
    "\n",
    "Conditioning is the behavior of a perturbed problem mathematically (analytically).  Stability is concerned with how an algorithm behaves when perturbed (say with input)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditioning and Condition Numbers\n",
    "\n",
    "A **well-conditioned** problem is one where a small perturbation to the original problem leads to only small changes in the solution.  Formally we can think of a function $f$ which maps $x$ to $y$\n",
    "$$\n",
    "    f(x) = y ~~~~\\text{or}~~~~ f: X \\rightarrow Y.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using $\\epsilon$ notation, we can think of $x \\in X$ where we perturb $x$ with $\\epsilon$ and we ask how the result $y$ changes, in more formal and familiar notation the question can be termed as\n",
    "$$\n",
    "    ||f(x) - f(x + \\epsilon)|| \\leq C ||x - (x+\\epsilon)||\n",
    "$$\n",
    "for some constant $C$ possible dependent on $\\epsilon$ depending on the type of conditioning we are considering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Absolute Condition Number\n",
    "\n",
    "If we let $\\delta x = x + \\epsilon$ be the small perturbation to the input and $\\delta f = f(x + \\delta x) - f(x)$ be the result the **absolute condition number** $\\hat{\\kappa}$ can be defined as\n",
    "$$\n",
    "    \\hat{\\kappa} = \\sup_{\\delta x} \\frac{||\\delta f||}{||\\delta x||}\n",
    "$$\n",
    "for most problems (assuming $\\delta f$ and $\\delta x$ are both infinitesimal).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When $f$ is differentiable we can evaluate the condition number via the Jacobian as we did with Lipschitz constants in the homework (note that the Lipschitz constant is really a form of condition number).  Recall that the derivative of a multi-valued function can be termed in the form of a Jacobian $J(x)$ where\n",
    "$$\n",
    "    [J(x)]_{ij} = \\frac{\\partial f_i}{\\partial x_j}(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This allows us to write the infinitesimal $\\delta f$ as\n",
    "$$\n",
    "    \\delta f \\approx J(x) \\delta x\n",
    "$$\n",
    "with equality when $||\\delta x|| \\rightarrow 0$.  Then we can write the condition number as\n",
    "$$\n",
    "    \\hat{\\kappa} = ||J(x)||\n",
    "$$\n",
    "where the norm is the one induced by the spaces $X$ and $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Relative Condition Number\n",
    "\n",
    "The **relative condition number** is defined similarly and is related to the difference before between the absolute error and relative error as defined previously.  With the same caveats as before it can be defined as\n",
    "$$\n",
    "    \\kappa = \\sup_{\\delta x} \\left( \\frac{\\frac{||\\delta f||}{||f(x)||}}{\\frac{||\\delta x||}{||x||}} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again if $f$ is differentiable we can use the Jacobian $J(x)$ to evaluate the relative condition number as\n",
    "$$\n",
    "    \\kappa = \\frac{||J(x)||}{||f(x)|| ~/ ~||x||}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples\n",
    "Calculate the following relative condition numbers of the following problems.\n",
    "\n",
    "1. $\\sqrt{x}$ for $x > 0$.\n",
    "1. Obtain the scalar $f(x) = x_1 - x_2$ from the vector $x = (x_1, x_2)^T \\in \\mathbb R^2$ using a $\\ell_\\infty$ norm.\n",
    "1. $\\tan x$ for $x$ near $10^100$.\n",
    "1. Calculation of the eigenvalues of \n",
    "$$\n",
    "    \\begin{bmatrix} 1 & 1000 \\\\ x & 1 \\end{bmatrix}\n",
    "$$\n",
    "where we are perturbing around $x = 0$.\n",
    "1. Matrix-vector multiplication $Ax$ only perturbing $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Condition Number of a Matrix\n",
    "\n",
    "The condition number of a matrix is defined by the product\n",
    "$$\n",
    "    \\kappa(A) = ||A||~||A^{-1}||.\n",
    "$$\n",
    "where here we are thinking about the matrix rather than a problem.  If $\\kappa$ is small than $A$ is said to be **well-conditioned**.  If $A$ is singular we assign $\\kappa(A) = \\infty$ as the matrix's condition number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When we are considering the $\\ell_2$ norm then we can write the condition number as\n",
    "$$\n",
    "    \\kappa(A) = \\frac{\\sqrt{\\rho(A^\\ast A)}}{\\sqrt{\\rho((A^\\ast A)^{-1})}} = \\frac{\\sqrt{\\max |\\lambda|}}{\\sqrt{\\min |\\lambda|}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Condition Number of a System of Equations\n",
    "\n",
    "Another way to think about the conditioning of a problem we have looked at before is that the matrix $A$ itself is an input to the problem.  Consider than the system of equations $Ax = b$ where we will perturb both $A$ and $x$ resulting in\n",
    "$$\n",
    "    (A + \\delta A)(x + \\delta x) = b.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Assuming we solve the problem exactly we know that $Ax = b$ and that the infinitesimals multiplied $\\delta A \\delta x$ is small the above simplifies to\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    (A + \\delta A)(x + \\delta x) &= b \\\\\n",
    "    Ax + \\delta Ax + A \\delta x + \\delta A \\delta x &= b \\\\\n",
    "    \\delta Ax + A \\delta x & = 0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Solving for $\\delta x$ leads to \n",
    "$$\n",
    "    \\delta x = -A^{-1} \\delta A x\n",
    "$$\n",
    "implying\n",
    "$$\n",
    "    ||\\delta x|| \\leq ||A^{-1}|| ||\\delta A|| ||x||\n",
    "$$\n",
    "and therefore\n",
    "$$\n",
    "    \\frac{\\frac{||\\delta x||}{||x||}}{\\frac{||\\delta A||}{||A||}} \\leq ||A^{-1}||~||A|| = \\kappa(A).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also say the following regarding the condition number of a system of equations then\n",
    "\n",
    "**Theorem:**  Let $b$ be fixed and consider the problem of computing $x$ in $Ax = b$ where $A$ is square and non-singular.  The condition number of this problem with respect to perturbations in $A$ is the condition number of the matrix $\\kappa(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stability\n",
    "\n",
    "We now return to the consideration of the fact that we are interested not only in the well-conditioning of a mathematical problem but in how we might solve it on a finite precision machine.  In some sense conditioning describes how well we can solve a problem in exact arithmetic and stability how well we can solve the problem in finite arithmetic.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Accuracy and Stability\n",
    "\n",
    "As we have defined before we will consider **absolute error** as\n",
    "$$\n",
    "    ||\\hat{f}(x) - f(x)||\n",
    "$$\n",
    "where $\\hat{f}(x)$ is the approximation to the true solution $f(x)$.  Similarly we can define **relative error** as\n",
    "$$\n",
    "    \\frac{||\\hat{f}(x) - f(x)||}{||f(x)||}.\n",
    "$$\n",
    "In the ideal case we would like the relative error to be $\\mathcal{O}(\\epsilon_{\\text{machine}})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Looking back at our definitions of conditioning it is clear that it unrealistic to expect a poorly conditioned problem to be accurately computed.  We then constrict our notion of stability with the notion of conditioning so that a problem $f$ is **stable** for $x \\in X$\n",
    "$$\n",
    "    \\frac{||\\hat{f}(x) - f(\\hat{x})||}{||f(\\hat{x})||} = \\mathcal{O}(\\epsilon_{\\text{machine}})\n",
    "$$\n",
    "for some $\\hat{x}$ with \n",
    "$$\n",
    "    \\frac{||\\hat{x} - x||}{||x||} = \\mathcal{O}(\\epsilon_{\\text{machine}}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words\n",
    "> A stable algorithm gives nearly the right answer to nearly the right question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Backwards Stability\n",
    "\n",
    "A stronger notion of stability can also be defined which is satisfied by many approaches in numerical linear algebra.  We say that an algorithm $\\hat{f}$ is **backward stable** if for $x \\in X$ we have\n",
    "$$\n",
    "    \\hat{f}(x) = f(\\hat{x})\n",
    "$$\n",
    "for some $\\hat{x}$ with\n",
    "$$\n",
    "    \\frac{||\\hat{x} - x||}{||x||} = \\mathcal{O}(\\epsilon_{\\text{machine}}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words\n",
    "> A backward stable algorithm gives exactly the right answer to nearly the right question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An important aspect of the above statement is that we can not necessarily guarantee an accurate result.  If the condition number $\\kappa(x)$ is small we would expect that a stable algorithm would give us an accurate result (by definition).  This is reflected in the following theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Theorem:**  Suppose a backward stable algorithm is applied to solve a problem $f: X \\rightarrow Y$ with condition number $\\kappa$ on a finite precision machine, then the relative errors satisfy\n",
    "$$\n",
    "    \\frac{||\\hat{f}(x) - f(\\hat{x})||}{||f(\\hat{x})||} = \\mathcal{O}(\\kappa(x) ~ \\epsilon_{\\text{machine}}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Proof:**  By the definition the condition number of a problem we can write\n",
    "$$\n",
    "    \\frac{||\\hat{f}(x) - f(\\hat{x})||}{||f(\\hat{x})||} \\leq (\\kappa(x) + \\mathcal{o}(1))\\frac{||\\hat{x} - x||}{||x||}\n",
    "$$\n",
    "where $\\mathcal{o}(1) \\rightarrow 0$ as $\\epsilon_{\\text{machine}} \\rightarrow 0$.  Combining this with the definition of backwards stability we can arrive at the statement of the theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The proof above demonstrates **backward error analysis**, in other words using the condition number of the problem and stability of the algorithm to determine the error.  A perhaps more obvious approach to determine eventual accuracy would be to consider the accrual of error at each step of an algorithm given slightly perturbed input.  This approach is known as **forward error analysis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Stability of $Ax = b$ using Householder Triangularization\n",
    "\n",
    "As an example lets consider the conditioning and algorithm for solving $Ax = b$.  Here we will use a $QR$ factorization approach to solve $Ax = b$ given by a Householder triangularization.  First off lets discuss the $QR$ factorization itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Theorem:** Let the $QR$ factorization $A = QR$ of a matrix $A \\in \\mathbb C^{m \\times n}$ be computed using a Householder triangularization approach on a finite precision machine, then\n",
    "$$\n",
    "    \\hat{Q}\\hat{R} = A + \\delta A ~~~~~ \\frac{||\\delta A||}{||A||} = \\mathcal{O}(\\epsilon_{\\text{machine}})\n",
    "$$\n",
    "for some $\\delta A \\in \\mathbb C^{m \\times n}$ where $\\hat{Q}$ and $\\hat{R}$ are the finite arithmetic versions of $Q$ and $R$.  Householder triangularization is therefore backward stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Solving $Ax = b$ with $QR$ Factorization**\n",
    "\n",
    "So Householder triangularization is backwards stable but we also know that this does not guarantee accuracy if the problem itself is ill-conditioned.  Is backward stability enough to guarantee accurate results if we use it for $Ax = b$ for instance?  It turns out that the accuracy of the product of $QR$ is enough to guarantee accuracy of a larger algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the steps to solving $A x = b$ using $QR$ factorization:\n",
    "1. Compute the $QR$ factorization of $A$\n",
    "1. Multiply the vector $b$ by $Q^\\ast$ so that $y = Q^\\ast b$.\n",
    "1. Solve using backward-substitution the triangular system $R x = y$ or $x = R^{-1} y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We know that step (1) is backward stable, what about step (2), the matrix-vector multiplication?  We can write the estimate of the backwards stability as\n",
    "$$\n",
    "    (\\hat{Q} + \\delta Q) \\hat{y} = b ~~~~ \\text{with}~~~~ ||\\delta Q|| = \\mathcal{O}(\\epsilon_{\\text{machine}})\n",
    "$$\n",
    "where we have inverted the matrix $\\hat{Q}$ since it is unitary.  Since this is exact we know also that the matrix-vector multiplication is also backwards stable since this is an equivalent statement to multiplying $b$ by a slightly perturbed matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Step (3) is backward substitution (or the computation of $R^{-1}$).  Writing the backwards stability estimate we have\n",
    "$$\n",
    "    (\\hat{R} + \\delta R) \\hat{x} = \\hat{y} ~~~~\\text{with}~~~~ \\frac{||\\delta R||}{||\\hat{R}||} = \\mathcal{O}(\\epsilon_{\\text{machine}})\n",
    "$$\n",
    "demonstrating that the results $\\hat{x}$ is the exact solution to a slight perturbation of the original problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These results lead to the following two theorems:\n",
    "\n",
    "**Theorem:**  Using $QR$ factorization to solve $Ax=b$ as described above is backward stable, satisfying\n",
    "$$\n",
    "    (A + \\Delta A) \\hat{x} = b, ~~~~ \\frac{||\\Delta A||}{||A||} = \\mathcal{O}(\\epsilon_{\\text{machine}})\n",
    "$$\n",
    "for some $\\Delta A \\in \\mathbb C^{m \\times n}$.\n",
    "\n",
    "**Theorem:** The solution $\\hat{x}$ computed by the above algorithm satisfies\n",
    "$$\n",
    "    \\frac{||\\hat{x} - x||}{||x||} = \\mathcal{O}(\\kappa(x) ~ \\epsilon_{\\text{machine}}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenproblems\n",
    "\n",
    "### Overview\n",
    "### Hessenberg and Tridiogonal form\n",
    "### Rayleigh Quotient\n",
    "### Inverse Iteration\n",
    "### QR Algorithm\n",
    "### Alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Methods\n",
    "\n",
    "### Arnoldi Iteration\n",
    "### GMRES\n",
    "### Lanczos Iteration\n",
    "### Conjugate Gradients\n",
    "### Biorthogonalization Methods\n",
    "### Preconditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
